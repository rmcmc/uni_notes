# Linear Models

## Intro

Categorical variables are called factors, they may be binary or have levels

y may be reffered to as dependnent or response variable(s)

x may be called independent , explanatory, predictor variables.

We use $n$ to denote observations

We use $r$ for the number of explanatory variables.

A model might look like

$y_i = \beta_0 + \beta_1x_{i1} + ... \beta_{ir} + \epsilon_i$

The $\beta_0 + \beta_1x_{i1} + ... \beta_{ir}$ is reffered to as the linear predictor. Epsilon is the random error.

When r=2 we fit a plane! Then hyper-planes in higher dimensions.

Remember a linear model is linear in the parameters (betas), it is allowed to include logs, quadratics.

It is easier to work in matrix notation. Parameter vector:

$\boldsymbol{\beta} = (\beta_0, \beta_1, ...)^T$

The $\boldsymbol{x} = (x_1, x_2...)^T$ is combined with a vector of ones to create the design matrix $\boldsymbol{X}$.

This combined creates

$\boldsymbol{y} = 
\boldsymbol{X}
\boldsymbol{\beta}+
\boldsymbol{\epsilon}$

$\boldsymbol{X}$ has the shape $n \times p$


## Chapter 4 - Hypothesis testing

The most natural and simple hypothesis is whether a given $\beta$ is equal to 0. It has no effect on the model.

However there might be more general null hypothesis. Eg:

- $\beta_1 = \beta_2 = \beta_3 = 0$
- $\beta_1 = \beta_2$

To cover all bases (for linear hypothesis) however we can say:

$H_0 : \boldsymbol{C\beta} = c$

$H_A : \boldsymbol{C\beta} \neq c$ (At least one is not equal.)

C is $q \times p$ and c is $q \times 1$ of known constants. C has rank q, so full rank. So at each row of C and c we are asserting some hypothesis that the linear combination of C is equal to c. With this framework it is not possible to specify one-sided tests, though this is rarely of interest.

### Some eamples of C and c

$H_0 : \beta_1 = 1, \beta_2=2$

$C =
\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix} =
\begin{pmatrix}
1 \\
2
\end{pmatrix}$

\

$H_0 : \beta_1 = \beta_2 = 0$
 
$C =
\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix} =
\begin{pmatrix}
0 \\
0
\end{pmatrix}$

\

$H_0 : \beta_2 = 0$
 
$C =
\begin{pmatrix}
0 & 0 & 1
\end{pmatrix} =
\begin{pmatrix}
0 
\end{pmatrix}$

\

$H_0 : \beta_2 = 3$
 
$C =
\begin{pmatrix}
0 & 0 & 1
\end{pmatrix} =
\begin{pmatrix}
3
\end{pmatrix}$

\

$H_0 : \beta_1 = \beta_2$ Which is equivalent to $H_0 : \beta_1 - \beta_2 = 0$
 
$C =
\begin{pmatrix}
0 & 1 & -1
\end{pmatrix} =
\begin{pmatrix}
0
\end{pmatrix}$

### Test Stat  q > 1

$$\frac{(C \hat{\beta} -c )^T(C(X^TX)^{-1}C^T)^{-1}(C \hat{\beta} -c )}{q\hat{\sigma}^2} \sim F_{q, n-p}$${#eq-f-test}

Note : This is a one-sided test so $1-\alpha$ (0.95) not  $1-\alpha / 2$ (0.975, 95% CI). This is due to square terms.

### Test Stat  q = 1

When there is only one test eg. $\beta_1=0$ then the abiove equation can be simplified to a t-test. This becomes:

$\frac{\hat{\beta}_i - c_i}{\hat{\sigma} \sqrt{g_{ii}}} \sim t_{n-p}$

Where $G = (X^TX)^{-1}$ and $g_{ii}$ is the *i*-th diagonal term.

### Some notes

The r `summary` will tell you $\hat{\sigma}$ through the "Residual Standard Error" line

The F-Statistic in `summary` is testing whether all coefficients other than intercept are 0

Pay little attention to tests of the individual coefficients if doing multi coeefficent tests. Significance can easily flip between all and individuals. Even if all are indicating p>0.05 and test says p>0.05.

### Nested models

By nested models we mean comparing one model to another, where one of them contains a subset of the other. It is usually used to determine whether there is value in including the term in the term(s) in the model.

We are trying to trade off increasing regression sum of squares vs excluding the term. So there is value in verifying whether something should be included.

An example is:

$y= \beta_0 + \beta_1x_1$ vs $y= \beta_0 + \beta_1x_1 + \beta_2 x^2$

So does $\beta_2 = 0$?

For nested models we use a similar framework to before but modify the $\beta$ vector. The C matrix becomes a 0 matrix with the indentity matrix appended to the right where the $I_n$ has an n of $p_f - p_r$ where $p_f$ is parameter in the full model and $p_r$ is the number of params in the reduced model.

In $\boldsymbol{\beta}$ we have a stack of vectors $(\boldsymbol{\beta}_1, \boldsymbol{\beta}_2)^T$. Where $(\boldsymbol{\beta}_1$ is a $p_r \times 1$ and $(\boldsymbol{\beta}_2$ is a $(p_f - p_r) \times 1$.

Element order is arbituary provided that we are consistent between all matrices and vectors.

A useful summary is via ANOVA tables where:

Source of Variation | Sum of Squares (SS) | DoF | Mean Square (MS) | Mean Square Ratio (MSR)
--------------------|---------------------|-----|------------------|-------------------------
Due to $X_1$ is $\beta_2$ = 0 | $S_1$ | $p_r$ | $S_1 / p_r$ | $F_1$
Due to $X_2$ only | $S_2$ | $p_f - p_r$ | $S_2 /(p_f - p_r)$ | $F_2$
Residual | $S_r$ | $n -p_f$ | $\hat{\sigma}^2$ | $F_2$
Total   | $y^Ty$ | n | |

The MSR are calculated by dividing the variance of each model by the product of the Dof and residuals so:

$F_2 = \frac{S_2}{(p_f-p_r)\hat{\sigma}^2}$

$F_1 = \frac{S_1}{p_r\hat{\sigma}^2}$

We typically work through these tests in sequence, firstly test 2 then test 1. 

The initial test on $F_2$ is just the test in @eq-f-test. Where the null is $\beta_2 = 0$ and the null dist is $F_{p_f-p_r, n-p_f}$ (remember it's one sided) and we test if $F_2 >F_{p_f-p_r, n-p_f}$. $F_2$ can also be calculated from residual sum of squares of models (RSS,, which can be obtained from ANOVA summary in R).

$$F_2 = \frac{(RRS_r - RSS_f) / (p_f - p_r)}{(RSS_f)/(n-p_f)}$$ {#eq-exam-f}

After $\beta_2$ we test the hypothesis that $\beta_1=0$ given we know $\beta_1$ is 0, or $\boldsymbol{\beta}=\textbf{0}$ (this is a little handwavy as the test doesn't poove the null, but it is accepted convention).

We test $F_1 > F_{p_r, n-p_f}$

### Minimal model

A special test based on the previous section. If we set $c=0$ and $C=I$ we would test that every single coefficient is equal to zero. This basically tests that y is 0 and constant irrespetive of the variables. What we really want to test howeveris that y has a floating mean, but is constant. 

Therefore we test that the regressors only are zero by saying $\beta_1 = \beta_0$ and that the $\beta_2$ is just all the regressors, making a p-1 length. The $\beta_0$ model is called the minimal or null model, with a mean of $\beta_0$ and variance of $\sigma^2$. As we don't really care if $beta_0$ is 0 or not we exclude it from the ANOVA tables.

TODO: read this https://stats.stackexchange.com/questions/256726/linear-regression-what-does-the-f-statistic-r-squared-and-residual-standard-err

So in this model $p_r=1$ and $p_f = p-1$

It can be show (see notes p29) that:

Source of Variation | Sum of Squares (SS) | DoF | Mean Square (MS) | Mean Square Ratio (MSR)
--------------------|---------------------|-----|------------------|-------------------------
Due to regressors | $S_2 = \hat{\beta}^TX^TX\hat{\beta} - n \bar{y}^2$ | $p-1$ | $S_2 / (p-1)$ | $F$
Residual | $S_r = y^Ty -\hat{\beta}^TX^TX\hat{\beta}$ | $n -p$ | $\hat{\sigma}^2$ | 
Total   | $S_{yy} =y^Ty - n\bar{y}^2$ | n-1 | |

So $F = \frac{S_2}{(p-1)\hat{\sigma}^2}$ where null is $F_{p-1, n-p}$

TODO: what are they talking about top of p30

### ANOVA - Application in R

> This is highly likley to be examined

With r you have two choices for ANOVA one where you enter each model seperately where one model is the baseline and the other is more complex so :

- $y = \beta_0 + \beta_1x_1$ (mdl_1)
- $y = \beta_0 + \beta_1x_1 + \beta_2x_1^2$ (mdl_2)

so `anova(mdl_1, mdl_2)` or you simply run `anova(mdl_2)`.

In the first option it automatically detects the extra terms and assigns those to $\beta_2$ and then $H_0: \beta_2 = 0$ is assessed. Values from the summary of this can readily be placed into @eq-exam-f.

TODO : Down from the blue arrow

## Chapter 5

Remember:

$\epsilon_i = y_i - \textbf{x}_i^T\boldsymbol{\beta}$

We assume that $\epsilon_i$ is:

- 0 mean, there is no systematic error from $\textbf{X}\boldsymbol{\beta}$
- Independent
- Common variance - homoscedasticity
- Normaly distributed

We check these through residuals (the estimate of the error based on fitted model). If the model were to be correct they would be normally distributed, however they are not independent or with common variance. Unequal variances can be corrected for by standardised residuals:

$s_i \frac{e_i}{\sqrt{\hat{\text{Var}}(e_i)}} \sim t_{n-p}$ where $\hat{\text{Var}}(e_i)$ is the estimate of variance

>  observed residuals are not independent and do not have equal variances

### q-q plot

Plots quantiles of observed vs expected and you should get a straight line, you may plot:

- Standard Normal against observed data
- Normal with mean and variance of observed against observed data
- Standardised residual 

You should see a straight line of plots if the fit is appropriate. If:

- The plot is bowed there is skew
- Down left/Up right = heavier tails

Histograms should be avoided if the dataset is small.

### Homoscedasticity

Plot a scatter graph of residual against the fitted value (y predicted). This should be a uniform band. Typically issues arise where the results fan outwards to the larger values

### Independence

Typically we plot the residual in observed order, through an "index plot". The order is based on SME knowledge (time, distance,etc). We are looking for trends, closely linked order points, etc.

### Formal testing

We can formally test whether the residuals contain outliers by standardising the residuals and looking for points. If the standardised residuals ($s_i$) are t distributed we can assess how "likely" they are by assessing the quantiles.

However if we were to do this for every point we are multiple testing and so a corrrection to the cut off should be applied.Formally:

$|s_i| > t_{n - p, 1-\alpha/2}$

Instead of the bonferonni test we could use the &Scaron;id&aacute;k correction which is:

> $\alpha^* = 1 - (1-\alpha)^{1/n}$, where $\alpha^*$ is the adjusted stat.

## Chapter 6 - Interactions and Factors

We include factors through the use of dummy or indicator variables. Where the are two levels this will simply be a 0/1. For more than one catergory, one-hot encoding will be used.

With dummy variables you have to be very careful not to over parameterise the model. For instance for a binary factor you could either:

- Remove $\beta_0$ and have $\alpha_1$ and $\alpha_2$ in place. Indicated by two seperate columns of 0/1 in the design matrix.
- Set a global $\mu$