# Linear Models

> The variance of residual ($e$, $\sigma^2M$) is not the same as the variance of random error term ($\epsilon$, $\sigma^2I_n$)

## Intro

Categorical variables are called factors, they may be binary or have levels

y may be reffered to as dependnent or response variable(s)

x may be called independent , explanatory, predictor variables.

We use $n$ to denote observations

We use $r$ for the number of explanatory variables.

A model might look like

$y_i = \beta_0 + \beta_1x_{i1} + ... \beta_{ir} + \epsilon_i$

The $\beta_0 + \beta_1x_{i1} + ... \beta_{ir}$ is reffered to as the linear predictor. Epsilon is the random error.

When r=2 we fit a plane! Then hyper-planes in higher dimensions.

Remember a linear model is linear in the parameters (betas), it is allowed to include logs, quadratics.

It is easier to work in matrix notation. Parameter vector:

$\boldsymbol{\beta} = (\beta_0, \beta_1, ...)^T$

The $\boldsymbol{x} = (x_1, x_2...)^T$ is combined with a vector of ones to create the design matrix $\boldsymbol{X}$.

This combined creates

$\boldsymbol{y} = 
\boldsymbol{X}
\boldsymbol{\beta}+
\boldsymbol{\epsilon}$

$\boldsymbol{X}$ has the shape $n \times p$

$\boldsymbol{\beta}$ has the shape $p \times 1$

In linear modelling we assume that the $\epsilon$:

- Is a MV Norm
- 0 Mean
- Independent
- Common variance $\sigma^2$ (homoscedasticity)

So :

$\epsilon \sim N_n(0, \sigma^2 I_n)$

And if $y = X\beta + \epsilon$ then we can say by linear transformation that

$y \sim N_n(X\beta, \sigma^2 I_n)$


In this course:

- Explanatory - the base $x$ variable, from the data
- Regressor - The constant, the $\beta x $ and any function or transform of $x$ with a coefficient. 

So $y = \beta_0 + \beta_1 x \beta_2 x^2$ has a single explantory variable, but 3 regressors.

## Chaprter 2 - Fit, estimate and residuals

Once fitted the the difference between the observed actual and fitted values ($x_i^T\hat{\beta}$) is the residual. The vector of residuals therefore can be calced as:

$e = y - X\hat{\beta}$

The **sum of squared of the residuals** or **residual su of squares** ($S_r$) is very important

$S_r = S(\hat{\boldsymbol{\beta}}) = \sum_{i=1}^n e_i^2 =\textbf{e}^T\textbf{e} = (y - X\hat{\beta})^T(y - X\hat{\beta})$

> In linalg rememeber that $x^Tx$ of a columnar vector is equivalent to $\sum x^2_i$

This concept is very useful in linear modelling because through the MVNorm we can show that:

$L(\beta, \sigma^2 ; y) = f(y|\beta. \sigma^2) \propto \sigma^{-n}\exp(- \frac{1}{2\sigma^2}(y - X\hat{\beta})^T(y - X\hat{\beta}))$

A key part of this derivation is that $|\sigma^2I_n| = (\sigma^2)^n$. The log likelihood can be shown to be 

$\ell(\beta, \sigma^2;y) = -nlog(\sigma) - \frac{1}{2\sigma^2}(y - X\hat{\beta})^T(y - X\hat{\beta}) + c$

To maximise likelihood with respect to $\beta$ therefore we should be looking to minimise $S_r$. Minimising this is the **least squares** method, which both mathematically and intuitively makes sense. There is an important consequence!

> Assuming X has rank p, the least squares estimator of $\beta$ is $\hat{\beta} = (X^TX)^{-1}X^Ty$

When finding the MLE of $\sigma^2$ we find $\frac{S_r}{n}$, however this is a biased estimator and instead we use $\frac{S_r}{n-p}$

### Estimator mean and variance

Given $y \sim N_n(X\beta, \sigma^2I_n)$

Uisng this we can show that $\hat{\beta}$ is an unbiased estimator

$E(\hat{\beta}) = E((X^TX)^{-1}X^Ty)$ where $E(y) =  X\beta$ so

$E((X^TX)^{-1}X^TX\beta)$ , the $X^TX$ terms cancel by inverse leaving $\beta$

Furthermore $Var(\hat{\beta})$ can be calculated:

$Var(\hat{\beta}) = Var((X^TX)^{-1}X^Ty)$ where $Var(y) =  \sigma^2I_n$

Recall that given a non-stochastic matrix $A$, and stochastic y, then $Var(Ay) = A Var(y) A^T$. From matrix laws too $(AB)^T = B^TA^T$ and $(A^{-1})^T = (A^T)^{-1}$

$Var(\hat{\beta}) = Var((X^TX)^{-1}X^Ty) =  (X^TX)^{-1}X^T Var(y)((X^TX)^{-1}X^T)^T$

$= (X^TX)^{-1}X^T Var(y) X((X^TX)^{-1})^T = (X^TX)^{-1}X^T Var(y) X((X^TX)^T)^{-1})$

$= (X^TX)^{-1}X^T Var(y) X(X^TX)^{-1}$

$= (X^TX)^{-1}X^T \sigma^2 I_n X(X^TX)^{-1}$

$= \sigma^2  (X^TX)^{-1}X^TX(X^TX)^{-1} = \sigma^2 (X^TX)^{-1}$


> $\hat{\beta} = N_p(\beta, \sigma^2 (X^TX)^{-1})$

### Error variance estimate


$e = y-X\beta = y - X(X^TX)^{-1}X^Ty$, the $y$ can be factorised out to give

$e = My$, where $M = I_n - X(X^TX)^{-1}X^T$

If $E(e) = E(My) = ME(y) = MX\beta$

$MX = (I_n - X(X^TX)^{-1}X^T)X = X - X(X^TX)^{-1}X^TX = 0$

So $MX = 0$ therefore $E(e) = 0$ . M is also **idempotent**, which means $M^2 = M$. idempotent matrices (except $I_N$) are always singular.

Another important result of M is that:

$Var(e) = Var(My) = MVar(y)M^T = M\sigma^2I_nM =\sigma^2M^2 = \sigma^2M$

So $M$ is related to the variance-covariance matrix of the residuals. 

>The variance of an individual residual is $\sigma^2$ times the corresponding diagonal of $M$

It can be shown (using trace and rank rules) that:

$\hat{\sigma}^2 = \frac{1}{n-p}\sum^n_{i=1}e_i^2 = \frac{1}{n-p}S_r$

The square root of this is the **residual standard error**, this is given in the r output.

It can also be shown by probability theory that a MVNorm with covariance matrix $M$, that $\frac{S_r}{\sigma^2} \sim \chi^2_{n-p}$

### \hat{\beta} covariance

Rememeber that:

$corr(\beta_0, \beta_1) = \frac{Cov(\hat{\beta_0},\hat{\beta_1})}{se(\hat{\beta_0}) \times se(\hat{\beta_1)}}$

## Model Fit : Coefficient of determination $R^2$

$S_r$ can be thought of as a measure of fit. However it will vary depending magnitudes of the variables, so cannot be compared directly between different problems. To enable better comparison we standardise with $S_{yy}$ (Total sum of squares), $R^2$ may also be called the coefficient of determination.

$R^2 = \frac{S_{yy} - S_r}{S_{yy}}$ where $S_{yy} = (y-\bar{y})^T(y-\bar{y}) = y^Ty - n\bar{y}^2$

In simpler terms:

- $R^2$ is the proportion of the total sum of squares that the model explains
- Whilst $S_r$ is the explained part of $S_{yy}$

$S_{yy}$ may also be written as $SS_{Total}$

Relying on $R^2$ alone for model comparison is not sensible as it will always increase when parameters are added. The use of adjusted $R^2$ is preffereable as it takes into account the number of parameters.

$R^2(adj) = 1 - \frac{S_r/(n-p)}{S_{yy}/(n-1)}$


## Confidence and Prediction Intervals



## Chapter 4 - Hypothesis testing

The most natural and simple hypothesis is whether a given $\beta$ is equal to 0. It has no effect on the model.

However there might be more general null hypothesis. Eg:

- $\beta_1 = \beta_2 = \beta_3 = 0$
- $\beta_1 = \beta_2$

To cover all bases (for linear hypothesis) however we can say:

$H_0 : \boldsymbol{C\beta} = c$

$H_A : \boldsymbol{C\beta} \neq c$ (At least one is not equal.)

C is $q \times p$ and c is $q \times 1$ of known constants. C has rank q, so full rank. So at each row of C and c we are asserting some hypothesis that the linear combination of C is equal to c. With this framework it is not possible to specify one-sided tests, though this is rarely of interest.

### Some eamples of C and c

$H_0 : \beta_1 = 1, \beta_2=2$

$C =
\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix} =
\begin{pmatrix}
1 \\
2
\end{pmatrix}$

\

$H_0 : \beta_1 = \beta_2 = 0$
 
$C =
\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix} =
\begin{pmatrix}
0 \\
0
\end{pmatrix}$

\

$H_0 : \beta_2 = 0$
 
$C =
\begin{pmatrix}
0 & 0 & 1
\end{pmatrix} =
\begin{pmatrix}
0 
\end{pmatrix}$

\

$H_0 : \beta_2 = 3$
 
$C =
\begin{pmatrix}
0 & 0 & 1
\end{pmatrix} =
\begin{pmatrix}
3
\end{pmatrix}$

\

$H_0 : \beta_1 = \beta_2$ Which is equivalent to $H_0 : \beta_1 - \beta_2 = 0$
 
$C =
\begin{pmatrix}
0 & 1 & -1
\end{pmatrix} =
\begin{pmatrix}
0
\end{pmatrix}$

### Test Stat  q > 1

$$\frac{(C \hat{\beta} -c )^T(C(X^TX)^{-1}C^T)^{-1}(C \hat{\beta} -c )}{q\hat{\sigma}^2} \sim F_{q, n-p}$${#eq-f-test}

Note : This is a one-sided test so $1-\alpha$ (0.95) not  $1-\alpha / 2$ (0.975, 95% CI). This is due to square terms.

### Test Stat  q = 1

When there is only one test eg. $\beta_1=0$ then the abiove equation can be simplified to a t-test. This becomes:

$\frac{\hat{\beta}_i - c_i}{\hat{\sigma} \sqrt{g_{ii}}} \sim t_{n-p}$

Where $G = (X^TX)^{-1}$ and $g_{ii}$ is the *i*-th diagonal term.

### Some notes

The r `summary` will tell you $\hat{\sigma}$ through the "Residual Standard Error" line

The F-Statistic in `summary` is testing whether all coefficients other than intercept are 0

Pay little attention to tests of the individual coefficients if doing multi coeefficent tests. Significance can easily flip between all and individuals. Even if all are indicating p>0.05 and test says p>0.05.

### Nested models

By nested models we mean comparing one model to another, where one of them contains a subset of the other. It is usually used to determine whether there is value in including the term in the term(s) in the model.

We are trying to trade off increasing regression sum of squares vs excluding the term. So there is value in verifying whether something should be included.

An example is:

$y= \beta_0 + \beta_1x_1$ vs $y= \beta_0 + \beta_1x_1 + \beta_2 x^2$

So does $\beta_2 = 0$?

For nested models we use a similar framework to before but modify the $\beta$ vector. The C matrix becomes a 0 matrix with the indentity matrix appended to the right where the $I_n$ has an n of $p_f - p_r$ where $p_f$ is parameter in the full model and $p_r$ is the number of params in the reduced model.

In $\boldsymbol{\beta}$ we have a stack of vectors $(\boldsymbol{\beta}_1, \boldsymbol{\beta}_2)^T$. Where $(\boldsymbol{\beta}_1$ is a $p_r \times 1$ and $(\boldsymbol{\beta}_2$ is a $(p_f - p_r) \times 1$.

Element order is arbituary provided that we are consistent between all matrices and vectors.

A useful summary is via ANOVA tables where:

Source of Variation | Sum of Squares (SS) | DoF | Mean Square (MS) | Mean Square Ratio (MSR)
--------------------|---------------------|-----|------------------|-------------------------
Due to $X_1$ is $\beta_2$ = 0 | $S_1$ | $p_r$ | $S_1 / p_r$ | $F_1$
Due to $X_2$ only | $S_2$ | $p_f - p_r$ | $S_2 /(p_f - p_r)$ | $F_2$
Residual | $S_r$ | $n -p_f$ | $\hat{\sigma}^2$ | $F_2$
Total   | $y^Ty$ | n | |

The MSR are calculated by dividing the variance of each model by the product of the Dof and residuals so:

$F_2 = \frac{S_2}{(p_f-p_r)\hat{\sigma}^2}$

$F_1 = \frac{S_1}{p_r\hat{\sigma}^2}$

We typically work through these tests in sequence, firstly test 2 then test 1. 

The initial test on $F_2$ is just the test in @eq-f-test. Where the null is $\beta_2 = 0$ and the null dist is $F_{p_f-p_r, n-p_f}$ (remember it's one sided) and we test if $F_2 >F_{p_f-p_r, n-p_f}$. $F_2$ can also be calculated from residual sum of squares of models (RSS,, which can be obtained from ANOVA summary in R).

$$F_2 = \frac{(RRS_r - RSS_f) / (p_f - p_r)}{(RSS_f)/(n-p_f)}$$ {#eq-exam-f}

After $\beta_2$ we test the hypothesis that $\beta_1=0$ given we know $\beta_1$ is 0, or $\boldsymbol{\beta}=\textbf{0}$ (this is a little handwavy as the test doesn't poove the null, but it is accepted convention).

We test $F_1 > F_{p_r, n-p_f}$

### Minimal model

A special test based on the previous section. If we set $c=0$ and $C=I$ we would test that every single coefficient is equal to zero. This basically tests that y is 0 and constant irrespetive of the variables. What we really want to test howeveris that y has a floating mean, but is constant. 

Therefore we test that the regressors only are zero by saying $\beta_1 = \beta_0$ and that the $\beta_2$ is just all the regressors, making a p-1 length. The $\beta_0$ model is called the minimal or null model, with a mean of $\beta_0$ and variance of $\sigma^2$. As we don't really care if $beta_0$ is 0 or not we exclude it from the ANOVA tables.

TODO: read this https://stats.stackexchange.com/questions/256726/linear-regression-what-does-the-f-statistic-r-squared-and-residual-standard-err

So in this model $p_r=1$ and $p_f = p-1$

It can be show (see notes p29) that:

Source of Variation | Sum of Squares (SS) | DoF | Mean Square (MS) | Mean Square Ratio (MSR)
--------------------|---------------------|-----|------------------|-------------------------
Due to regressors | $S_2 = \hat{\beta}^TX^TX\hat{\beta} - n \bar{y}^2$ | $p-1$ | $S_2 / (p-1)$ | $F$
Residual | $S_r = y^Ty -\hat{\beta}^TX^TX\hat{\beta}$ | $n -p$ | $\hat{\sigma}^2$ | 
Total   | $S_{yy} =y^Ty - n\bar{y}^2$ | n-1 | |

So $F = \frac{S_2}{(p-1)\hat{\sigma}^2}$ where null is $F_{p-1, n-p}$

TODO: what are they talking about top of p30

### ANOVA - Application in R

> This is highly likley to be examined

With r you have two choices for ANOVA one where you enter each model seperately where one model is the baseline and the other is more complex so :

- $y = \beta_0 + \beta_1x_1$ (mdl_1)
- $y = \beta_0 + \beta_1x_1 + \beta_2x_1^2$ (mdl_2)

so `anova(mdl_1, mdl_2)` or you simply run `anova(mdl_2)`.

In the first option it automatically detects the extra terms and assigns those to $\beta_2$ and then $H_0: \beta_2 = 0$ is assessed. Values from the summary of this can readily be placed into @eq-exam-f.

TODO : Down from the blue arrow

## Chapter 5

Remember:

$\epsilon_i = y_i - \textbf{x}_i^T\boldsymbol{\beta}$

We assume that $\epsilon_i$ is:

- 0 mean, there is no systematic error from $\textbf{X}\boldsymbol{\beta}$
- Independent
- Common variance - homoscedasticity
- Normaly distributed

We check these through residuals (the estimate of the error based on fitted model). If the model were to be correct they would be normally distributed, however they are not independent or with common variance. Unequal variances can be corrected for by standardised residuals:

$s_i \frac{e_i}{\sqrt{\hat{\text{Var}}(e_i)}} \sim t_{n-p}$ where $\hat{\text{Var}}(e_i)$ is the estimate of variance

>  observed residuals are not independent and do not have equal variances

### q-q plot

Plots quantiles of observed vs expected and you should get a straight line, you may plot:

- Standard Normal against observed data
- Normal with mean and variance of observed against observed data
- Standardised residual 

You should see a straight line of plots if the fit is appropriate. If:

- The plot is bowed there is skew
- Down left/Up right = heavier tails

Histograms should be avoided if the dataset is small.

### Homoscedasticity

Plot a scatter graph of residual against the fitted value (y predicted). This should be a uniform band. Typically issues arise where the results fan outwards to the larger values

### Independence

Typically we plot the residual in observed order, through an "index plot". The order is based on SME knowledge (time, distance,etc). We are looking for trends, closely linked order points, etc.

### Formal testing

We can formally test whether the residuals contain outliers by standardising the residuals and looking for points. If the standardised residuals ($s_i$) are t distributed we can assess how "likely" they are by assessing the quantiles.

However if we were to do this for every point we are multiple testing and so a corrrection to the cut off should be applied.Formally:

$|s_i| > t_{n - p, 1-\alpha/2}$

Instead of the bonferonni test we could use the &Scaron;id&aacute;k correction which is:

> $\alpha^* = 1 - (1-\alpha)^{1/n}$, where $\alpha^*$ is the adjusted stat.

## Chapter 6 - Interactions and Factors

We include factors through the use of dummy or indicator variables. Where the are two levels this will simply be a 0/1. For more than one catergory, one-hot encoding will be used.

With dummy variables you have to be very careful not to over parameterise the model.

You cannot have a model constant and a constant for each factor this is overparameterisation and leads to multicolinearity. A  matrix is not full rank when there are linear combinations. Therefore $X^TX$ is no longer invertable (singular). Therefore we cannot calculate $\hat{\beta}$ through $(X^TX)^{-1}X^T y$.

The solution is to move to the following representation:

$y_{ij} = \mu + \alpha_i + \epsilon_{ij}$

This can be implement one of two ways

#### Level One Zero Constraint

Also known as **corner points constraints**. Essentially the first catergorcial variable is used for $\mu$ and the first $\alpha_1$ is set to zero removing it from the model and the design matrix.

All other $\alpha_i$ i=(2,3,...k) are therefore deviations from the mean ($\mu$) and hence how the mean of y changes as we move between factors.

`options(contrast=c(factor= ..., ordered =...))` will implement this in R

#### Sum to Zero

An alternative where all of the $\alpha$ values are designed to sum to zero. In a one factor model the $\mu$ is therefore the overall mean, while $\alpha$ is the factor group means deviation from the this. 

This method does remove a parameter, it is achievd by accounting for the sum of all alphas from 1 to k-1 and setting k equal to the negative sum of these so they cancel. X therefore loses it's $\alpha_k$ column, but also modifies all other $alpha_i$ columns where the **row** is i=k

TODO: This is a bit lost on me, see p46/47

### Two way factors

In the two way factor you have rows and columns. In the corner point constraint the mean is set by the top left mean. Then an \alpha$ deviation is reported for 2,3,...k rows and a $\beta$ for 2,3,...k columns.

### Interactions

Interactions are non additive effects of one variable on anaother, this is most commonly achieved through multiplication. These terms can be tested for significance using `summary` or nested comparisions conducted with `anova`. Care must be taken with factors as it is easy to get high number of combinations, inflate $R^2$ through overfitting or get muktiplicity problems with signiificance. 

> When you can an interaction in `R` you are not just creating a mutliplying term, but also including that interaction terms as standalone variables themselves