# GLMs - Generalized Linear Models

An excellent intro can be read here:

https://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models


EDA

https://oakleyj.github.io/MAS61004/eda-for-logistic-regression.html

## Chapter 1 - Linear Model Refresh

In linear models we make a number of assumption about the error term $\epsilon$ these may not always hold:

- The distribution of data may be skewed or counts based (not normal)
- Data may not be independent, this is offten the case when assessing things over time
- Data may not have constant variance

GLMs and Random Effects models allow us to adapt to these violations. Some problems associated with fitting a linear model when LM assumptions are violated:

- You exceed reasonable bounds expected by the problem
- You invalidate all your tests. Being gaussian leads to nice properties that mean F and t tests are possible. That is no longer the case.

## Chapter 2 - Logistic Regression - Part 1

We use a link function between the mean ($\mu$) and the linear predictor to provide constraints to the mean and introduce non-linearity.

In linear models we assume that $Y \sim N(X\beta, \sigma^2 I)$. In GLMs the decouple the mean and distribution of $Y$ so $E(Y_i)=\mu_i$.

In binary cases data is either 1 or 0, which is Bernoulli distributed, and $P(Y_i =1) = \mu_i$ the parameter in the bernoulli case is between 0 and 1. The use of the a linear model here wouldn't make sense as we would push $\mu_i$ out of the bounds [0,1]. As this is bernoulli the expectation is $E(Y_i) = \mu_i$. 

As $x_i^T\beta$ can very between between $(-\infty, + \infty)$ then a mapping is required to bring it down to [0,1], or map $\mu$ up to $(-\infty, + \infty)$.

In this case the **logit** link function:

$\text{logit}(\mu_i) = log(\frac{\mu_i}{1 - \mu_i}) = x_i^T\beta$

As $P(Y_i =1) = \mu_i$ then $P(Y_i = 0) = 1 - \mu_i$ then the logged term are the odds. It therefore follws that:

$\mu_i = \frac{\exp(x_i^T\beta)}{1 + exp(x_i^T\beta)} = h(x_i^T\beta)$

$h$ is the **inverse link function**.

TODO: Expand this based on p13

Due to it's relationship to the binomial we now have a glm:

- Inverse Link
- $E(Y_i) = \mu_i = h(x_i^T\beta)$
- $Var(Y_i) = \frac{\mu_i(1- \mu_i)}{n_i}$

> Note that the variance is coupled with $\mu$ therefore as the covariates change the variance is allowed to change too, so we can move away from the fixed variance assumption

### Link function

We chose a logit above but there are a number of functions that can map to [0,1]. They must however have an inverse i.e be monotone.

Another eaxmple is the **probit** 


## GLM - Maths and Theory

### Mean

Let $\eta = x_i^T\beta$ equal the linear predictor, and $E(Y_i) = \mu_i$. Then:

$g(\mu_i) = \eta_i$ and $g$ is the **Link**

$h(\eta_i) = \mu_i$ and $h$ is the **Inverse Link**

$g$ must be monotonic and invertable. It enables the means to be non-linear in the parameters.

> The link function converts the mean to the linear predictor. The inverse link converts the linear predictor to the mean.

### Variance

The variance is $Var(Y_i) = \frac{\phi}{w_i} V(\mu_i)$ where:

- $\phi$ is a scalar called the scale or dispersion parameter and depends on the distribution (it may be known or require estimating and can be used when over dispersion is present)
- $w_i$ are the weights
- $V(\mu_i)$ is the Variance Function

The variance can now be non-constant and change with the mean

> The variance does not depend on the link fiunction

### PDF/PMF

The general form is given as 

$f_i(y: \theta_i, \phi) = \exp\{ w_i \frac{y\theta_i - b(\theta_i)}{\phi} + c(y,\phi)\}$

For a single observation :

$f(y: \theta, \phi) = \exp\{ \frac{y\theta - b(\theta)}{\phi / w} + c(y,\phi)\}$

And:

$E(Y_i) = b'(\theta)$

$Var(Y_i) = \frac{\phi}{w}b''(\theta)$

($\theta$ is used as in terms of $\mu$ the pdf and moments would become less elegant)

### Exponential family

There are a number of distributions that belong to the exponential family, and it has a useful, tractable properties. These included Normal, Poisson, Binomial, Gamma and more. Their properties allow us to calculate the E and Var as before.

### Common exponential forms

Note the binomial version here is where $Y$ is the proportion $[0,1]$

| Distribution | $\phi$ | $w$ | $b'(\theta) =\mu$ | $b''(\theta)$ |  $b(\theta)$ | $c(y,\phi)$ |
|-------------------|--------|-----|--------------|---------------|--------------|-------------|
| Normal $Y\sim N(\theta, \phi)$ | $\phi$ | 1 | $\theta$ | 1 | $\theta^2 /2$ | $-(y^2 / \phi +log(2\pi\phi))/2$|
| Poisson $Y\sim Po(e^{\theta})$ | 1 | 1 | $e^{\theta}$ | $e^{\theta}$ | $e^{\theta}$ | $-log(y!)$|
| Binomial $nY\sim Bin(n, e^{\theta}/(e^{\theta} +1)$ | 1 | n | $e^{\theta}/(e^{\theta} +1)$ | $\mu(1-\mu)$ | $log(e^{\theta} +1)$ | $log{n \choose ny}$|

### Canonical Link

The canonical link for a distribution is the one in which $\theta_i  = x_i^T\beta$ or $g=(\b'^{-1})$

** MUST PRACTICE THIS, see task 3**

### 3.3 Estimation

IRLS is implemented in the IRLS notebook

TODO - Do section






