[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This book is my personal notes for my stats course. It is very rough!\nThis book has been created with Quarto through VSCode. I highly recommend it."
  },
  {
    "objectID": "linear_models.html#intro",
    "href": "linear_models.html#intro",
    "title": "5  Linear Models",
    "section": "5.1 Intro",
    "text": "5.1 Intro\nCategorical variables are called factors, they may be binary or have levels\ny may be reffered to as dependnent or response variable(s)\nx may be called independent , explanatory, predictor variables.\nWe use \\(n\\) to denote observations\nWe use \\(r\\) for the number of explanatory variables.\nA model might look like\n\\(y_i = \\beta_0 + \\beta_1x_{i1} + ... \\beta_{ir} + \\epsilon_i\\)\nThe \\(\\beta_0 + \\beta_1x_{i1} + ... \\beta_{ir}\\) is reffered to as the linear predictor. Epsilon is the random error.\nWhen r=2 we fit a plane! Then hyper-planes in higher dimensions.\nRemember a linear model is linear in the parameters (betas), it is allowed to include logs, quadratics.\nIt is easier to work in matrix notation. Parameter vector:\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, ...)^T\\)\nThe \\(\\boldsymbol{x} = (x_1, x_2...)^T\\) is combined with a vector of ones to create the design matrix \\(\\boldsymbol{X}\\).\nThis combined creates\n\\(\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}\\)\n\\(\\boldsymbol{X}\\) has the shape \\(n \\times p\\)\n\\(\\boldsymbol{\\beta}\\) has the shape \\(p \\times 1\\)\nIn linear modelling we assume that the \\(\\epsilon\\):\n\nIs a MV Norm\n0 Mean\nIndependent\nCommon variance \\(\\sigma^2\\) (homoscedasticity)\n\nSo :\n\\(\\epsilon \\sim N_n(0, \\sigma^2 I_n)\\)\nAnd if \\(y = X\\beta + \\epsilon\\) then we can say by linear transformation that\n\\(y \\sim N_n(X\\beta, \\sigma^2 I_n)\\)\nIn this course:\n\nExplanatory - the base \\(x\\) variable, from the data\nRegressor - The constant, the $x $ and any function or transform of \\(x\\) with a coefficient.\n\nSo \\(y = \\beta_0 + \\beta_1 x \\beta_2 x^2\\) has a single explantory variable, but 3 regressors."
  },
  {
    "objectID": "linear_models.html#chaprter-2---fit-estimate-and-residuals",
    "href": "linear_models.html#chaprter-2---fit-estimate-and-residuals",
    "title": "5  Linear Models",
    "section": "5.2 Chaprter 2 - Fit, estimate and residuals",
    "text": "5.2 Chaprter 2 - Fit, estimate and residuals\nOnce fitted the the difference between the observed actual and fitted values (\\(x_i^T\\hat{\\beta}\\)) is the residual. The vector of residuals therefore can be calced as:\n\\(e = y - X\\hat{\\beta}\\)\nThe sum of squared of the residuals or residual su of squares (\\(S_r\\)) is very important\n\\(S_r = S(\\hat{\\boldsymbol{\\beta}}) = \\sum_{i=1}^n e_i^2 =\\textbf{e}^T\\textbf{e} = (y - X\\hat{\\beta})^T(y - X\\hat{\\beta})\\)\n\nIn linalg rememeber that \\(x^Tx\\) of a columnar vector is equivalent to \\(\\sum x^2_i\\)\n\nThis concept is very useful in linear modelling because through the MVNorm we can show that:\n\\(L(\\beta, \\sigma^2 ; y) = f(y|\\beta. \\sigma^2) \\propto \\sigma^{-n}\\exp(- \\frac{1}{2\\sigma^2}(y - X\\hat{\\beta})^T(y - X\\hat{\\beta}))\\)\nA key part of this derivation is that \\(|\\sigma^2I_n| = (\\sigma^2)^n\\). The log likelihood can be shown to be\n\\(\\ell(\\beta, \\sigma^2;y) = -nlog(\\sigma) - \\frac{1}{2\\sigma^2}(y - X\\hat{\\beta})^T(y - X\\hat{\\beta}) + c\\)\nTo maximise likelihood with respect to \\(\\beta\\) therefore we should be looking to minimise \\(S_r\\). Minimising this is the least squares method, which both mathematically and intuitively makes sense. There is an important consequence!\n\nAssuming X has rank p, the least squares estimator of \\(\\beta\\) is \\(\\hat{\\beta} = (X^TX)^{-1}X^Ty\\)\n\nWhen finding the MLE of \\(\\sigma^2\\) we find \\(\\frac{S_r}{n}\\), however this is a biased estimator and instead we use \\(\\frac{S_r}{n-p}\\)\n\n5.2.1 Estimator mean and variance\nGiven \\(y \\sim N_n(X\\beta, \\sigma^2I_n)\\)\nUisng this we can show that \\(\\hat{\\beta}\\) is an unbiased estimator\n\\(E(\\hat{\\beta}) = E((X^TX)^{-1}X^Ty)\\) where \\(E(y) = X\\beta\\) so\n\\(E((X^TX)^{-1}X^TX\\beta)\\) , the \\(X^TX\\) terms cancel by inverse leaving \\(\\beta\\)\nFurthermore \\(Var(\\hat{\\beta})\\) can be calculated:\n\\(Var(\\hat{\\beta}) = Var((X^TX)^{-1}X^Ty)\\) where \\(Var(y) = \\sigma^2I_n\\)\nRecall that given a non-stochastic matrix \\(A\\), and stochastic y, then \\(Var(Ay) = A Var(y) A^T\\). From matrix laws too \\((AB)^T = B^TA^T\\) and \\((A^{-1})^T = (A^T)^{-1}\\)\n\\(Var(\\hat{\\beta}) = Var((X^TX)^{-1}X^Ty) = (X^TX)^{-1}X^T Var(y)((X^TX)^{-1}X^T)^T\\)\n\\(= (X^TX)^{-1}X^T Var(y) X((X^TX)^{-1})^T = (X^TX)^{-1}X^T Var(y) X((X^TX)^T)^{-1})\\)\n\\(= (X^TX)^{-1}X^T Var(y) X(X^TX)^{-1}\\)\n\\(= (X^TX)^{-1}X^T \\sigma^2 I_n X(X^TX)^{-1}\\)\n\\(= \\sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1} = \\sigma^2 (X^TX)^{-1}\\)\n\n\\(\\hat{\\beta} = N_p(\\beta, \\sigma^2 (X^TX)^{-1})\\)\n\nTODO:2.5, 2.6"
  },
  {
    "objectID": "linear_models.html#chapter-4---hypothesis-testing",
    "href": "linear_models.html#chapter-4---hypothesis-testing",
    "title": "5  Linear Models",
    "section": "5.5 Chapter 4 - Hypothesis testing",
    "text": "5.5 Chapter 4 - Hypothesis testing\nThe most natural and simple hypothesis is whether a given \\(\\beta\\) is equal to 0. It has no effect on the model.\nHowever there might be more general null hypothesis. Eg:\n\n\\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\)\n\\(\\beta_1 = \\beta_2\\)\n\nTo cover all bases (for linear hypothesis) however we can say:\n\\(H_0 : \\boldsymbol{C\\beta} = c\\)\n\\(H_A : \\boldsymbol{C\\beta} \\neq c\\) (At least one is not equal.)\nC is \\(q \\times p\\) and c is \\(q \\times 1\\) of known constants. C has rank q, so full rank. So at each row of C and c we are asserting some hypothesis that the linear combination of C is equal to c. With this framework it is not possible to specify one-sided tests, though this is rarely of interest.\n\n5.5.1 Some eamples of C and c\n\\(H_0 : \\beta_1 = 1, \\beta_2=2\\)\n\\(C = \\begin{pmatrix} 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\)\n\n\n\\(H_0 : \\beta_1 = \\beta_2 = 0\\)\n\\(C = \\begin{pmatrix} 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\)\n\n\n\\(H_0 : \\beta_2 = 0\\)\n\\(C = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\end{pmatrix}\\)\n\n\n\\(H_0 : \\beta_2 = 3\\)\n\\(C = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\end{pmatrix}\\)\n\n\n\\(H_0 : \\beta_1 = \\beta_2\\) Which is equivalent to \\(H_0 : \\beta_1 - \\beta_2 = 0\\)\n\\(C = \\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\end{pmatrix}\\)\n\n\n5.5.2 Test Stat q > 1\n\\[\\frac{(C \\hat{\\beta} -c )^T(C(X^TX)^{-1}C^T)^{-1}(C \\hat{\\beta} -c )}{q\\hat{\\sigma}^2} \\sim F_{q, n-p} \\tag{5.1}\\]\nNote : This is a one-sided test so \\(1-\\alpha\\) (0.95) not \\(1-\\alpha / 2\\) (0.975, 95% CI). This is due to square terms.\n\n\n5.5.3 Test Stat q = 1\nWhen there is only one test eg. \\(\\beta_1=0\\) then the abiove equation can be simplified to a t-test. This becomes:\n\\(\\frac{\\hat{\\beta}_i - c_i}{\\hat{\\sigma} \\sqrt{g_{ii}}} \\sim t_{n-p}\\)\nWhere \\(G = (X^TX)^{-1}\\) and \\(g_{ii}\\) is the i-th diagonal term.\n\n\n5.5.4 Some notes\nThe r summary will tell you \\(\\hat{\\sigma}\\) through the “Residual Standard Error” line\nThe F-Statistic in summary is testing whether all coefficients other than intercept are 0\nPay little attention to tests of the individual coefficients if doing multi coeefficent tests. Significance can easily flip between all and individuals. Even if all are indicating p>0.05 and test says p>0.05.\n\n\n5.5.5 Nested models\nBy nested models we mean comparing one model to another, where one of them contains a subset of the other. It is usually used to determine whether there is value in including the term in the term(s) in the model.\nWe are trying to trade off increasing regression sum of squares vs excluding the term. So there is value in verifying whether something should be included.\nAn example is:\n\\(y= \\beta_0 + \\beta_1x_1\\) vs \\(y= \\beta_0 + \\beta_1x_1 + \\beta_2 x^2\\)\nSo does \\(\\beta_2 = 0\\)?\nFor nested models we use a similar framework to before but modify the \\(\\beta\\) vector. The C matrix becomes a 0 matrix with the indentity matrix appended to the right where the \\(I_n\\) has an n of \\(p_f - p_r\\) where \\(p_f\\) is parameter in the full model and \\(p_r\\) is the number of params in the reduced model.\nIn \\(\\boldsymbol{\\beta}\\) we have a stack of vectors \\((\\boldsymbol{\\beta}_1, \\boldsymbol{\\beta}_2)^T\\). Where \\((\\boldsymbol{\\beta}_1\\) is a \\(p_r \\times 1\\) and \\((\\boldsymbol{\\beta}_2\\) is a \\((p_f - p_r) \\times 1\\).\nElement order is arbituary provided that we are consistent between all matrices and vectors.\nA useful summary is via ANOVA tables where:\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares (SS)\nDoF\nMean Square (MS)\nMean Square Ratio (MSR)\n\n\n\n\nDue to \\(X_1\\) is \\(\\beta_2\\) = 0\n\\(S_1\\)\n\\(p_r\\)\n\\(S_1 / p_r\\)\n\\(F_1\\)\n\n\nDue to \\(X_2\\) only\n\\(S_2\\)\n\\(p_f - p_r\\)\n\\(S_2 /(p_f - p_r)\\)\n\\(F_2\\)\n\n\nResidual\n\\(S_r\\)\n\\(n -p_f\\)\n\\(\\hat{\\sigma}^2\\)\n\\(F_2\\)\n\n\nTotal\n\\(y^Ty\\)\nn\n\n\n\n\n\nThe MSR are calculated by dividing the variance of each model by the product of the Dof and residuals so:\n\\(F_2 = \\frac{S_2}{(p_f-p_r)\\hat{\\sigma}^2}\\)\n\\(F_1 = \\frac{S_1}{p_r\\hat{\\sigma}^2}\\)\nWe typically work through these tests in sequence, firstly test 2 then test 1.\nThe initial test on \\(F_2\\) is just the test in Equation 5.1. Where the null is \\(\\beta_2 = 0\\) and the null dist is \\(F_{p_f-p_r, n-p_f}\\) (remember it’s one sided) and we test if \\(F_2 >F_{p_f-p_r, n-p_f}\\). \\(F_2\\) can also be calculated from residual sum of squares of models (RSS,, which can be obtained from ANOVA summary in R).\n\\[F_2 = \\frac{(RRS_r - RSS_f) / (p_f - p_r)}{(RSS_f)/(n-p_f)} \\tag{5.2}\\]\nAfter \\(\\beta_2\\) we test the hypothesis that \\(\\beta_1=0\\) given we know \\(\\beta_1\\) is 0, or \\(\\boldsymbol{\\beta}=\\textbf{0}\\) (this is a little handwavy as the test doesn’t poove the null, but it is accepted convention).\nWe test \\(F_1 > F_{p_r, n-p_f}\\)\n\n\n5.5.6 Minimal model\nA special test based on the previous section. If we set \\(c=0\\) and \\(C=I\\) we would test that every single coefficient is equal to zero. This basically tests that y is 0 and constant irrespetive of the variables. What we really want to test howeveris that y has a floating mean, but is constant.\nTherefore we test that the regressors only are zero by saying \\(\\beta_1 = \\beta_0\\) and that the \\(\\beta_2\\) is just all the regressors, making a p-1 length. The \\(\\beta_0\\) model is called the minimal or null model, with a mean of \\(\\beta_0\\) and variance of \\(\\sigma^2\\). As we don’t really care if \\(beta_0\\) is 0 or not we exclude it from the ANOVA tables.\nTODO: read this https://stats.stackexchange.com/questions/256726/linear-regression-what-does-the-f-statistic-r-squared-and-residual-standard-err\nSo in this model \\(p_r=1\\) and \\(p_f = p-1\\)\nIt can be show (see notes p29) that:\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSum of Squares (SS)\nDoF\nMean Square (MS)\nMean Square Ratio (MSR)\n\n\n\n\nDue to regressors\n\\(S_2 = \\hat{\\beta}^TX^TX\\hat{\\beta} - n \\bar{y}^2\\)\n\\(p-1\\)\n\\(S_2 / (p-1)\\)\n\\(F\\)\n\n\nResidual\n\\(S_r = y^Ty -\\hat{\\beta}^TX^TX\\hat{\\beta}\\)\n\\(n -p\\)\n\\(\\hat{\\sigma}^2\\)\n\n\n\nTotal\n\\(S_{yy} =y^Ty - n\\bar{y}^2\\)\nn-1\n\n\n\n\n\nSo \\(F = \\frac{S_2}{(p-1)\\hat{\\sigma}^2}\\) where null is \\(F_{p-1, n-p}\\)\nTODO: what are they talking about top of p30\n\n\n5.5.7 ANOVA - Application in R\n\nThis is highly likley to be examined\n\nWith r you have two choices for ANOVA one where you enter each model seperately where one model is the baseline and the other is more complex so :\n\n\\(y = \\beta_0 + \\beta_1x_1\\) (mdl_1)\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2\\) (mdl_2)\n\nso anova(mdl_1, mdl_2) or you simply run anova(mdl_2).\nIn the first option it automatically detects the extra terms and assigns those to \\(\\beta_2\\) and then \\(H_0: \\beta_2 = 0\\) is assessed. Values from the summary of this can readily be placed into Equation 5.2.\nTODO : Down from the blue arrow"
  },
  {
    "objectID": "linear_models.html#chapter-5",
    "href": "linear_models.html#chapter-5",
    "title": "5  Linear Models",
    "section": "5.6 Chapter 5",
    "text": "5.6 Chapter 5\nRemember:\n\\(\\epsilon_i = y_i - \\textbf{x}_i^T\\boldsymbol{\\beta}\\)\nWe assume that \\(\\epsilon_i\\) is:\n\n0 mean, there is no systematic error from \\(\\textbf{X}\\boldsymbol{\\beta}\\)\nIndependent\nCommon variance - homoscedasticity\nNormaly distributed\n\nWe check these through residuals (the estimate of the error based on fitted model). If the model were to be correct they would be normally distributed, however they are not independent or with common variance. Unequal variances can be corrected for by standardised residuals:\n\\(s_i \\frac{e_i}{\\sqrt{\\hat{\\text{Var}}(e_i)}} \\sim t_{n-p}\\) where \\(\\hat{\\text{Var}}(e_i)\\) is the estimate of variance\n\nobserved residuals are not independent and do not have equal variances\n\n\n5.6.1 q-q plot\nPlots quantiles of observed vs expected and you should get a straight line, you may plot:\n\nStandard Normal against observed data\nNormal with mean and variance of observed against observed data\nStandardised residual\n\nYou should see a straight line of plots if the fit is appropriate. If:\n\nThe plot is bowed there is skew\nDown left/Up right = heavier tails\n\nHistograms should be avoided if the dataset is small.\n\n\n5.6.2 Homoscedasticity\nPlot a scatter graph of residual against the fitted value (y predicted). This should be a uniform band. Typically issues arise where the results fan outwards to the larger values\n\n\n5.6.3 Independence\nTypically we plot the residual in observed order, through an “index plot”. The order is based on SME knowledge (time, distance,etc). We are looking for trends, closely linked order points, etc.\n\n\n5.6.4 Formal testing\nWe can formally test whether the residuals contain outliers by standardising the residuals and looking for points. If the standardised residuals (\\(s_i\\)) are t distributed we can assess how “likely” they are by assessing the quantiles.\nHowever if we were to do this for every point we are multiple testing and so a corrrection to the cut off should be applied.Formally:\n\\(|s_i| > t_{n - p, 1-\\alpha/2}\\)\nInstead of the bonferonni test we could use the Šidák correction which is:\n\n\\(\\alpha^* = 1 - (1-\\alpha)^{1/n}\\), where \\(\\alpha^*\\) is the adjusted stat."
  },
  {
    "objectID": "linear_models.html#chapter-6---interactions-and-factors",
    "href": "linear_models.html#chapter-6---interactions-and-factors",
    "title": "5  Linear Models",
    "section": "5.7 Chapter 6 - Interactions and Factors",
    "text": "5.7 Chapter 6 - Interactions and Factors\nWe include factors through the use of dummy or indicator variables. Where the are two levels this will simply be a 0/1. For more than one catergory, one-hot encoding will be used.\nWith dummy variables you have to be very careful not to over parameterise the model.\nYou cannot have a model constant and a constant for each factor this is overparameterisation and leads to multicolinearity. A matrix is not full rank when there are linear combinations. Therefore \\(X^TX\\) is no longer invertable (singular). Therefore we cannot calculate \\(\\hat{\\beta}\\) through \\((X^TX)^{-1}X^T y\\).\nThe solution is to move to the following representation:\n\\(y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\\)\nThis can be implement one of two ways\n\n5.7.0.1 Level One Zero Constraint\nAlso known as corner points constraints. Essentially the first catergorcial variable is used for \\(\\mu\\) and the first \\(\\alpha_1\\) is set to zero removing it from the model and the design matrix.\nAll other \\(\\alpha_i\\) i=(2,3,…k) are therefore deviations from the mean (\\(\\mu\\)) and hence how the mean of y changes as we move between factors.\noptions(contrast=c(factor= ..., ordered =...)) will implement this in R\n\n\n5.7.0.2 Sum to Zero\nAn alternative where all of the \\(\\alpha\\) values are designed to sum to zero. In a one factor model the \\(\\mu\\) is therefore the overall mean, while \\(\\alpha\\) is the factor group means deviation from the this.\nThis method does remove a parameter, it is achievd by accounting for the sum of all alphas from 1 to k-1 and setting k equal to the negative sum of these so they cancel. X therefore loses it’s \\(\\alpha_k\\) column, but also modifies all other \\(alpha_i\\) columns where the row is i=k\nTODO: This is a bit lost on me, see p46/47\n\n\n5.7.1 Two way factors\nIn the two way factor you have rows and columns. In the corner point constraint the mean is set by the top left mean. Then an $ deviation is reported for 2,3,…k rows and a \\(\\beta\\) for 2,3,…k columns.\n\n\n5.7.2 Interactions\nInteractions are non additive effects of one variable on anaother, this is most commonly achieved through multiplication. These terms can be tested for significance using summary or nested comparisions conducted with anova. Care must be taken with factors as it is easy to get high number of combinations, inflate \\(R^2\\) through overfitting or get muktiplicity problems with signiificance."
  },
  {
    "objectID": "linear_models.html#model-fit-coefficient-of-determination-r2",
    "href": "linear_models.html#model-fit-coefficient-of-determination-r2",
    "title": "5  Linear Models",
    "section": "5.3 Model Fit : Coefficient of determination \\(R^2\\)",
    "text": "5.3 Model Fit : Coefficient of determination \\(R^2\\)\n\\(S_r\\) can be thought of as a measure of fit. However it will vary depending magnitudes of the variables, so cannot be compared directly between different problems. To enable better comparison we standardise with \\(S_{yy}\\) (Total sum of squares), \\(R^2\\) may also be called the coefficient of determination.\n\\(R^2 = \\frac{S_{yy} - S_r}{S_{yy}}\\) where \\(S_{yy} = (y-\\bar{y})^T(y-\\bar{y}) = y^Ty - n\\bar{y}^2\\)\nIn simpler terms:\n\n\\(R^2\\) is the proportion of the total sum of squares that the model explains\nWhilst \\(S_r\\) is the explained part of \\(S_{yy}\\)\n\n\\(S_{yy}\\) may also be written as \\(SS_{Total}\\)\nRelying on \\(R^2\\) alone for model comparison is not sensible as it will always increase when parameters are added. The use of adjusted \\(R^2\\) is preffereable as it takes into account the number of parameters.\n\\(R^2(adj) = 1 - \\frac{S_r/(n-p)}{S_{yy}/(n-1)}\\)"
  },
  {
    "objectID": "linear_models.html#confidence-and-prediction-intervals",
    "href": "linear_models.html#confidence-and-prediction-intervals",
    "title": "5  Linear Models",
    "section": "5.4 Confidence and Prediction Intervals",
    "text": "5.4 Confidence and Prediction Intervals"
  }
]